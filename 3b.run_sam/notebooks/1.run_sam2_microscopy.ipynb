{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook solves the cell tracking issue by using [SAM2](https://github.com/facebookresearch/segment-anything-2/tree/main) instead of the functionality within CellProfiler.\n",
    "Here I use the pretrained model to segment the nuclei in the video.\n",
    "The output is a mask for each object in each frame and the x,y coordinates centers of each object in each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that needs perfect conditions to work. \n",
    "With a GeForce RTX 3090 TI, the 24GB of VRAM sometimes are not enough to process the videos.\n",
    "\n",
    "Hold your breath, pick a four-leaf clover, avoid black cats, cracks, and mirrors, and let's go!\n",
    "\n",
    "This notebook is converted to a script and ran from script to be compatible with HPC cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents for this Notebook\n",
    "#### 1. Imports\n",
    "#### 2. Import data\n",
    "#### 3. get the masks and centers\n",
    "#### 4. Track multiple objects in the video\n",
    "#### 5. Track the objects through frames\n",
    "#### 6. Visualize the tracking and output the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top level imports\n",
    "import gc  # garbage collector\n",
    "import logging  # logging\n",
    "import pathlib  # path handling\n",
    "import shutil  # file handling\n",
    "import subprocess  # subprocess handling\n",
    "import sys  # system\n",
    "\n",
    "import lancedb  # lancedb database\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "import numpy as np  # numerical python\n",
    "import pandas as pd  # data handling\n",
    "import pyarrow as pa  # pyarrow for parquet\n",
    "import torch  # pytorch deep learning\n",
    "from csbdeep.utils import Path, normalize  # dependecy for stardist\n",
    "from PIL import Image  # image handling\n",
    "from sam2.build_sam import build_sam2, build_sam2_video_predictor  # sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor  # sam2 image predictor\n",
    "from skimage import io  # image handling\n",
    "from skimage.measure import label, regionprops  # coordinate handling\n",
    "from skimage.transform import resize  # image handling\n",
    "from stardist.models import StarDist2D  # stardist\n",
    "from stardist.plot import render_label  # stardist\n",
    "from torchvision import models  # pytorch models\n",
    "\n",
    "sys.path.append(\"../../utils/\")\n",
    "from SAM2_utils import (  # sam2 utils\n",
    "    delete_recorded_memory_history,\n",
    "    export_memory_snapshot,\n",
    "    generate_random_coords,\n",
    "    show_mask,\n",
    "    show_points,\n",
    "    start_record_memory_history,\n",
    "    stop_record_memory_history,\n",
    ")\n",
    "\n",
    "# check cuda devices\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"sam2_hiera_tiny.pt\": \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt\",\n",
    "    \"sam2_hiera_small.pt\": \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt\",\n",
    "    \"sam2_hiera_base_plus.pt\": \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\",\n",
    "    \"sam2_hiera_large.pt\": \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file using wget\n",
    "# this is the model checkpoint for the SAM2 model\n",
    "for file in models_dict.keys():\n",
    "    model_path = pathlib.Path(file).resolve()\n",
    "    new_model_path = pathlib.Path(\"../../data/models\").resolve() / model_path.name\n",
    "    # check if the model already exists\n",
    "    if not new_model_path.exists():\n",
    "        subprocess.run([\"wget\", models_dict[file]], check=True)\n",
    "        new_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.move(model_path, new_model_path)\n",
    "    else:\n",
    "        print(f\"Model {new_model_path} already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the model and the predictor\n",
    "sam2_checkpoint = pathlib.Path(\"../../data/models/sam2_hiera_tiny.pt\").resolve()\n",
    "model_cfg = \"sam2_hiera_t.yaml\"\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint)\n",
    "\n",
    "# set the path to the videos\n",
    "\n",
    "ordered_tiffs = pathlib.Path(\"../sam2_processing_dir/tiffs/\").resolve()\n",
    "converted_to_video_dir = pathlib.Path(\"../sam2_processing_dir/pngs/\").resolve()\n",
    "if converted_to_video_dir.exists():\n",
    "    shutil.rmtree(converted_to_video_dir)\n",
    "\n",
    "ordered_tiffs.mkdir(parents=True, exist_ok=True)\n",
    "converted_to_video_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_dir = pathlib.Path(\n",
    "    \"../../2.cellprofiler_ic_processing/illum_directory/20231017ChromaLive_6hr_4ch_MaxIP_test_small\"\n",
    ").resolve(strict=True)\n",
    "terminal_dir = pathlib.Path(\n",
    "    \"../../2.cellprofiler_ic_processing/illum_directory/20231017ChromaLive_endpoint_w_AnnexinV_2ch_MaxIP_test_small\"\n",
    ").resolve(strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the database object\n",
    "uri = pathlib.Path(\"../../data/objects_db\").resolve()\n",
    "db = lancedb.connect(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data formatted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of tiff files in the directory\n",
    "tiff_files = list(tiff_dir.glob(\"*.tiff\"))\n",
    "tiff_files = tiff_files + list(terminal_dir.glob(\"*.tiff\"))\n",
    "tiff_file_names = [file.stem for file in tiff_files]\n",
    "# files to df\n",
    "tiff_df = pd.DataFrame({\"file_name\": tiff_file_names, \"file_path\": tiff_files})\n",
    "\n",
    "# split the file_path column by _ but keep the original column\n",
    "tiff_df[\"file_name\"] = tiff_df[\"file_name\"].astype(str)\n",
    "tiff_df[[\"Well\", \"FOV\", \"Timepoint\", \"Z-slice\", \"Channel\", \"illum\"]] = tiff_df[\n",
    "    \"file_name\"\n",
    "].str.split(\"_\", expand=True)\n",
    "tiff_df[\"Well_FOV\"] = tiff_df[\"Well\"] + \"_\" + tiff_df[\"FOV\"]\n",
    "# drop all channels except for the first one\n",
    "tiff_df = tiff_df[tiff_df[\"Channel\"] == \"C01\"]\n",
    "tiff_df = tiff_df.drop(columns=[\"Channel\", \"illum\"])\n",
    "tiff_df[\"new_path\"] = (\n",
    "    str(ordered_tiffs)\n",
    "    + \"/\"\n",
    "    + tiff_df[\"Well_FOV\"]\n",
    "    + \"/\"\n",
    "    + tiff_df[\"file_name\"]\n",
    "    + \".tiff\"\n",
    ")\n",
    "tiff_df.reset_index(drop=True, inplace=True)\n",
    "tiff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the files to the new directory\n",
    "# from file path to new path\n",
    "for index, row in tiff_df.iterrows():\n",
    "    new_path = pathlib.Path(row[\"new_path\"])\n",
    "    new_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy(row[\"file_path\"], new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of directories in the ordered tiffs directory\n",
    "ordered_tiff_dirs = list(ordered_tiffs.glob(\"*\"))\n",
    "ordered_tiff_dir_names = [dir for dir in ordered_tiff_dirs]\n",
    "ordered_tiff_dir_names\n",
    "for dir in ordered_tiff_dir_names:\n",
    "    out_dir = converted_to_video_dir / dir.name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for tiff_file in dir.glob(\"*.tiff\"):\n",
    "        jpeg_file = pathlib.Path(f\"{out_dir}/{tiff_file.stem}.jpeg\")\n",
    "\n",
    "        if not jpeg_file.exists():\n",
    "            try:\n",
    "                with Image.open(tiff_file) as img:\n",
    "                    # Convert the image to 8-bit per channel\n",
    "                    img = img.convert(\"L\")\n",
    "                    img.save(jpeg_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to convert {tiff_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of dirs in the converted to video dir\n",
    "converted_dirs = list(converted_to_video_dir.glob(\"*\"))\n",
    "converted_dir_names = [dir for dir in converted_dirs]\n",
    "for dir in converted_dir_names:\n",
    "    dir = sorted(dir.glob(\"*.jpeg\"))\n",
    "    for i in enumerate(dir):\n",
    "        # rename the files to be in order\n",
    "        i[1].rename(f\"{dir[0].parent}/{str(i[0] + 1).zfill(3)}.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donwsample each frame to fit the images on the GPU - overwrite the copies JPEGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files in the directory\n",
    "converted_dirs_list = list(converted_to_video_dir.rglob(\"*\"))\n",
    "converted_dirs_list = [f for f in converted_dirs_list if f.is_file()]\n",
    "# posix path to string\n",
    "files = [str(f) for f in converted_dirs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to downscale to fit the model and images on the GPU\n",
    "# note that this is an arbitrary number and can be changed\n",
    "downscale_factor = 5\n",
    "# sort the files by name\n",
    "# downsample the image\n",
    "for f in files:\n",
    "    img = io.imread(f)\n",
    "    # downsample the image\n",
    "    downsampled_img = img[::downscale_factor, ::downscale_factor]\n",
    "    # save the downsampled image in place of the original image\n",
    "    io.imsave(f, downsampled_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get initial masks and centers via StarDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the first frame of each video\n",
    "### Set up a dict that holds the images path, the first frame_mask, and the first frame_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where one image set here is a single well and fov over all timepoints\n",
    "image_set_dict = {\n",
    "    \"image_set_name\": [],  # e.g. well_fov\n",
    "    \"image_set_path\": [],  # path to the directory\n",
    "    \"image_set_first_frame\": [],  # path to the first frame\n",
    "    \"image_x_y_coords\": [],  # list of x,y coordinates\n",
    "    \"image_labels\": [],  # list of labels for the x,y coordinates\n",
    "}\n",
    "\n",
    "# get the list of directories in the ordered tiffs directory\n",
    "dirs = list(converted_to_video_dir.glob(\"*\"))\n",
    "dirs = [dir for dir in dirs if dir.is_dir()]\n",
    "for dir in dirs:\n",
    "    # get the files in the directory\n",
    "    files = sorted(dir.glob(\"*.jpeg\"))\n",
    "    image_set_dict[\"image_set_name\"].append(dir.name)\n",
    "    image_set_dict[\"image_set_path\"].append(str(dir))\n",
    "    image_set_dict[\"image_set_first_frame\"].append(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the segementation\n",
    "Plot the following:\n",
    "- the original image\n",
    "- the segmentation\n",
    "- the x,y centers of the segmentation\n",
    "- the extracted masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StarDist2D.from_pretrained(\"2D_versatile_fluo\")\n",
    "\n",
    "# choose to visualize the results or not\n",
    "# best for troubleshooting or exploring the model\n",
    "visualize = False\n",
    "\n",
    "# loop through each image set and predict the instances\n",
    "for i in range(len(image_set_dict[\"image_set_name\"])):\n",
    "    print(\n",
    "        f\"{image_set_dict['image_set_name'][i]}: {image_set_dict['image_set_first_frame'][i]}\"\n",
    "    )\n",
    "    img = io.imread(image_set_dict[\"image_set_first_frame\"][i])\n",
    "    labels, _ = model.predict_instances(normalize(img))\n",
    "    # convert the labels into position coordinates\n",
    "    regions = regionprops(label(labels))\n",
    "    coords = np.array([r.centroid for r in regions])\n",
    "    coords = coords[:, [1, 0]]\n",
    "\n",
    "    if visualize:\n",
    "        # plot the points and the masks and the image side by side by side\n",
    "        fig, ax = plt.subplots(1, 4, figsize=(30, 15))\n",
    "        ax[0].imshow(img, cmap=\"gray\")\n",
    "        ax[0].set_title(\"Image\")\n",
    "        ax[1].imshow(render_label(labels, img=img))\n",
    "        ax[1].set_title(\"Masks\")\n",
    "        ax[2].imshow(img, cmap=\"gray\")\n",
    "        ax[2].scatter(\n",
    "            coords[:, 1],\n",
    "            coords[:, 0],\n",
    "            color=\"red\",\n",
    "            marker=\"*\",\n",
    "            s=100,\n",
    "            edgecolor=\"white\",\n",
    "            linewidth=1.25,\n",
    "        )\n",
    "        ax[2].set_title(\"Points\")\n",
    "\n",
    "        ax[3].invert_yaxis()\n",
    "        # make the aspect ratio equal\n",
    "        ax[3].set_aspect(\"equal\")\n",
    "        show_points(coords, np.ones(len(coords)), ax[3])\n",
    "    labels = np.ones(coords.shape[0], dtype=np.int32)\n",
    "    image_set_dict[\"image_x_y_coords\"].append(coords)\n",
    "    image_set_dict[\"image_labels\"].append(labels)\n",
    "\n",
    "# remove star dist model from memory\n",
    "del model\n",
    "# remove all stardist gpu memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Track multiple objects in the video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin GPU Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start recording memory snapshot history\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s:%(asctime)s %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level=logging.INFO)\n",
    "\n",
    "TIME_FORMAT_STR: str = \"%b_%d_%H_%M_%S\"\n",
    "# delete any prior memory profiling data\n",
    "delete_recorded_memory_history(\n",
    "    logger=logger, save_dir=pathlib.Path(\"../memory_snapshots/\").resolve()\n",
    ")\n",
    "\n",
    "# Keep a max of 100,000 alloc/free events in the recorded history\n",
    "# leading up to the snapshot.\n",
    "MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT: int = 100000\n",
    "start_record_memory_history(\n",
    "    logger=logger, max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_video_segments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each image set and predict the instances\n",
    "for i in range(len(image_set_dict[\"image_set_name\"])):\n",
    "    print(\n",
    "        f\"{image_set_dict['image_set_name'][i]}: {image_set_dict['image_set_first_frame'][i]}\"\n",
    "    )\n",
    "    frame_names = sorted(list(Path(image_set_dict[\"image_set_path\"][i]).glob(\"*.jpeg\")))\n",
    "    img = io.imread(frame_names[0])\n",
    "    h, w = img.shape\n",
    "    print(h, w)\n",
    "    # initialize the state\n",
    "    inference_state = predictor.init_state(\n",
    "        video_path=str(image_set_dict[\"image_set_path\"][i]),\n",
    "        offload_video_to_cpu=True,  # set to True if the video is too large to fit in GPU memory\n",
    "        offload_state_to_cpu=True,  # set to True if the state is too large to fit in GPU memory\n",
    "    )\n",
    "    predictor.reset_state(inference_state)\n",
    "    prompts = {}\n",
    "    ann_frame_idx = 0\n",
    "    ann_obj_idx = 1\n",
    "    samples = 1\n",
    "    negative_sampling = (\n",
    "        False  # set True to generate negative samples for better training\n",
    "    )\n",
    "    # loop through the points and add them to the state and get the masks\n",
    "    for _point, _label in zip(\n",
    "        image_set_dict[\"image_x_y_coords\"][i], image_set_dict[\"image_labels\"][i]\n",
    "    ):\n",
    "        _label = np.array([_label], dtype=np.int32)\n",
    "        _point = np.array([_point], dtype=np.float32)\n",
    "\n",
    "        if negative_sampling:\n",
    "            random_points, random_labels = generate_random_coords(\n",
    "                img=img, coords=_point, samples=samples\n",
    "            )\n",
    "            _point = np.concatenate([_point, random_points], axis=0)\n",
    "            _label = np.concatenate([_label, random_labels], axis=0)\n",
    "        # add the points to the state\n",
    "        _, out_obj_ids, out_mask_logits = predictor.add_new_points(\n",
    "            inference_state=inference_state,\n",
    "            frame_idx=ann_frame_idx,\n",
    "            obj_id=ann_obj_idx,\n",
    "            points=_point,\n",
    "            labels=_label,\n",
    "        )\n",
    "        # save the prompts\n",
    "        prompts[ann_obj_idx] = {\n",
    "            \"points\": _point,\n",
    "            \"labels\": _label,\n",
    "            \"out_obj_ids\": out_obj_ids[0],\n",
    "            \"out_mask_logits\": out_mask_logits[0].detach().cpu().numpy(),\n",
    "        }\n",
    "        # increment the object index for this frame\n",
    "        ann_obj_idx += 1\n",
    "\n",
    "    del prompts\n",
    "    del samples\n",
    "    # run propagation throughout the video and collect the results in a dict\n",
    "    video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n",
    "        inference_state\n",
    "    ):\n",
    "        video_segments[out_frame_idx] = {\n",
    "            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "            for i, out_obj_id in enumerate(range(1, ann_obj_idx))\n",
    "        }\n",
    "    stored_video_segments[image_set_dict[\"image_set_name\"][i]] = video_segments\n",
    "\n",
    "    # clear the memory\n",
    "    del inference_state\n",
    "\n",
    "    del out_mask_logits\n",
    "    del out_obj_ids\n",
    "    del out_frame_idx\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop GPU profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the memory snapshot to a file\n",
    "export_memory_snapshot(\n",
    "    logger=logger, save_dir=pathlib.Path(\"../memory_snapshots/\").resolve()\n",
    ")\n",
    "stop_record_memory_history(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove previous runs generated files\n",
    "# each of these directories will be created if they do not exist\n",
    "# the new files will be saved in these directories\n",
    "\n",
    "# for masks\n",
    "masks_dir = pathlib.Path(\"../sam2_processing_dir/masks\").resolve()\n",
    "if masks_dir.exists():\n",
    "    shutil.rmtree(masks_dir)\n",
    "masks_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# for gifs\n",
    "gifs_dir = pathlib.Path(\"../sam2_processing_dir/gifs\").resolve()\n",
    "if gifs_dir.exists():\n",
    "    shutil.rmtree(gifs_dir)\n",
    "gifs_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# for combined masks and tiffs\n",
    "combined_dir = pathlib.Path(\"../sam2_processing_dir/CP_input\").resolve()\n",
    "if combined_dir.exists():\n",
    "    shutil.rmtree(combined_dir)\n",
    "combined_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {\n",
    "    \"image_set_name\": [],\n",
    "    \"frame\": [],\n",
    "    \"object_id\": [],\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "    \"mask_path\": [],\n",
    "    \"mask_file_name\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each image set and save the predicted masks as images\n",
    "for i in range(len(image_set_dict[\"image_set_name\"])):\n",
    "    print(\n",
    "        f\"{image_set_dict['image_set_name'][i]}: {image_set_dict['image_set_first_frame'][i]}\"\n",
    "    )\n",
    "    frame_names = sorted(list(Path(image_set_dict[\"image_set_path\"][i]).glob(\"*.jpeg\")))\n",
    "    img = io.imread(frame_names[0])\n",
    "    h, w = img.shape\n",
    "    upscale_h = h * downscale_factor\n",
    "    upscale_w = w * downscale_factor\n",
    "    print(h, w, \"upscaled\", upscale_h, upscale_w)\n",
    "    # add all of the frames together for a rendered gif\n",
    "    # create a list of all the frames\n",
    "    frames = []\n",
    "\n",
    "    video_segments = stored_video_segments[image_set_dict[\"image_set_name\"][i]]\n",
    "    for out_frame_idx in range(0, len(frame_names), 1):\n",
    "        # create a figure\n",
    "        # set the frame path and make the directory if it doesn't exist\n",
    "        # create a frame image\n",
    "        frame_image = np.zeros((h, w), dtype=np.uint8)\n",
    "        # loop through the objects in the frame\n",
    "        for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "            # add the mask to the frame image\n",
    "            frame_image += (out_mask[0] * 255).astype(np.uint8)\n",
    "            out_mask = np.array(out_mask[0], dtype=np.float32)\n",
    "            # convert the outmask to an image\n",
    "            regions = regionprops(label(out_mask))\n",
    "            for region in regions:\n",
    "                y, x = region.centroid\n",
    "                # scale the x and y coordinates back to the original size\n",
    "                x = x * downscale_factor\n",
    "                y = y * downscale_factor\n",
    "                output_dict[\"frame\"].append(out_frame_idx)\n",
    "                output_dict[\"object_id\"].append(out_obj_id)\n",
    "                output_dict[\"x\"].append(x)\n",
    "                output_dict[\"y\"].append(y)\n",
    "                output_dict[\"mask_file_name\"].append(f\"{out_frame_idx}.png\")\n",
    "                output_dict[\"image_set_name\"].append(\n",
    "                    image_set_dict[\"image_set_name\"][i]\n",
    "                )\n",
    "                output_dict[\"mask_path\"].append(masks_dir)\n",
    "\n",
    "        # save the frame image\n",
    "        # scale the image upscale back to the original size\n",
    "        frame_image = Image.fromarray(frame_image)\n",
    "        frame_image = frame_image.resize((upscale_w, upscale_h), Image.NEAREST)\n",
    "\n",
    "        # convert the frame image to ints\n",
    "        frame_image_path = f\"{masks_dir}/{image_set_dict['image_set_name'][i]}_T{str(out_frame_idx + 1).zfill(4)}_Z0001_mask.png\"\n",
    "        frame_image.save(frame_image_path)\n",
    "\n",
    "        # add title to the subplot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        # show the image\n",
    "        ax.imshow(frame_image, cmap=\"gray\")\n",
    "        ax.set_title(f\"Frame {out_frame_idx}\")\n",
    "        # save the figure to a file\n",
    "        fig.savefig(f\"tmp_{out_frame_idx}.png\")\n",
    "        # close the figure\n",
    "        plt.close(fig)\n",
    "        # open the image\n",
    "        img = Image.open(f\"tmp_{out_frame_idx}.png\")\n",
    "        # append the image to the frames\n",
    "        frames.append(img)\n",
    "\n",
    "    fig_path = pathlib.Path(\n",
    "        f\"{gifs_dir}/{image_set_dict['image_set_name'][i]}_out.gif\"\n",
    "    ).resolve()\n",
    "    # save the frames as a gif\n",
    "    frames[0].save(\n",
    "        fig_path, save_all=True, append_images=frames[1:], duration=10, loop=0\n",
    "    )\n",
    "\n",
    "    # get all files that have tmp in the name\n",
    "    tmp_files = list(Path(\".\").glob(\"tmp*.png\"))\n",
    "    # delete all the tmp files\n",
    "    [f.unlink() for f in tmp_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_df = pd.DataFrame(output_dict)\n",
    "# add the mask file path\n",
    "file_paths_df[\"mask_file_path\"] = (\n",
    "    file_paths_df[\"mask_path\"].astype(str)\n",
    "    + \"/\"\n",
    "    + file_paths_df[\"mask_file_name\"].astype(str)\n",
    ")\n",
    "# type cast the columns\n",
    "file_paths_df[\"image_set_name\"] = file_paths_df[\"image_set_name\"].astype(str)\n",
    "file_paths_df[\"frame\"] = file_paths_df[\"frame\"].astype(np.int32)\n",
    "file_paths_df[\"object_id\"] = file_paths_df[\"object_id\"].astype(np.int32)\n",
    "file_paths_df[\"x\"] = file_paths_df[\"x\"].astype(np.float32)\n",
    "file_paths_df[\"y\"] = file_paths_df[\"y\"].astype(np.float32)\n",
    "file_paths_df[\"mask_path\"] = file_paths_df[\"mask_path\"].astype(str)\n",
    "file_paths_df[\"mask_file_name\"] = file_paths_df[\"mask_file_name\"].astype(str)\n",
    "file_paths_df[\"mask_file_path\"] = file_paths_df[\"mask_file_path\"].astype(str)\n",
    "# add to the db\n",
    "# set up schema\n",
    "schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"image_set_name\", pa.string()),\n",
    "        pa.field(\"frame\", pa.int32()),\n",
    "        pa.field(\"object_id\", pa.int32()),\n",
    "        pa.field(\"x\", pa.float32()),\n",
    "        pa.field(\"y\", pa.float32()),\n",
    "        pa.field(\"mask_path\", pa.string()),\n",
    "        pa.field(\"mask_file_name\", pa.string()),\n",
    "        pa.field(\"mask_file_path\", pa.string()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the table\n",
    "tbl = db.create_table(\"1.masked_images\", schema=schema, mode=\"overwrite\")\n",
    "# write the data to the table\n",
    "tbl.add(file_paths_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the table and check the first few rows\n",
    "tbl.to_pandas().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
